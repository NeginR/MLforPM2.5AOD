{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a template for usage of machine learning model in PM2.5 stimation with Aerosol Optical Depth and meteorological data. We used it for different datasets in different cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PART 1: DATA PREPARATION & EXPLORATORY ANALYSIS\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set publication-quality plot settings\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "mpl.rcParams['figure.figsize'] = (12, 8)\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['legend.title_fontsize'] = 14\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# 1. Load and inspect data\n",
    "print(\"Loading and inspecting data...\")\n",
    "df = pd.read_csv(\"../data/input.csv\", parse_dates=['date'], index_col=\"date\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Available features: {df.columns.tolist()}\")\n",
    "\n",
    "# Remove Season, Month, PM25lag and PM25lag2 columns if they exist\n",
    "columns_to_remove = ['S', 'M', 'PM25lag', 'PM25lag2']\n",
    "for col in columns_to_remove:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(col, axis=1)\n",
    "        print(f\"Removed {col} from dataset\")\n",
    "\n",
    "# 2. Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "print(\"\\nMissing values per feature:\")\n",
    "print(missing_data[missing_data > 0])\n",
    "print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Create a heatmap of missing values\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values in Dataset')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/missing_values.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Descriptive statistics\n",
    "print(\"\\nData summary statistics:\")\n",
    "desc_stats = df.describe().transpose()\n",
    "desc_stats['missing'] = df.isnull().sum()\n",
    "desc_stats['missing_percent'] = (df.isnull().sum() / len(df)) * 100\n",
    "print(desc_stats)\n",
    "\n",
    "# Save descriptive statistics to CSV for paper\n",
    "desc_stats.to_csv('../results/descriptive_statistics.csv')\n",
    "\n",
    "# 4. Outlier detection and removal\n",
    "def detect_and_remove_outliers(df, columns, method='zscore', threshold=3):\n",
    "    \"\"\"\n",
    "    Detect and remove outliers from specified columns using different methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input dataframe\n",
    "    columns : list\n",
    "        List of column names to check for outliers\n",
    "    method : str\n",
    "        Method to use for outlier detection ('zscore', 'iqr', or 'percentile')\n",
    "    threshold : float\n",
    "        Threshold for z-score method (default=3)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_clean : pandas DataFrame\n",
    "        DataFrame with outliers removed\n",
    "    outlier_info : dict\n",
    "        Information about removed outliers\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Skip columns with all NaN values\n",
    "        if df[col].isna().all():\n",
    "            continue\n",
    "            \n",
    "        outlier_info[col] = {'count_before': df[col].count()}\n",
    "        \n",
    "        if method == 'zscore':\n",
    "            # Z-score method\n",
    "            z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "            outliers = df[col].dropna()[z_scores > threshold].index\n",
    "            df_clean.loc[outliers, col] = np.nan\n",
    "            \n",
    "        elif method == 'iqr':\n",
    "            # IQR method\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
    "            df_clean.loc[outliers, col] = np.nan\n",
    "            \n",
    "        elif method == 'percentile':\n",
    "            # Percentile method (remove top and bottom 1%)\n",
    "            lower_bound = df[col].quantile(0.01)\n",
    "            upper_bound = df[col].quantile(0.99)\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
    "            df_clean.loc[outliers, col] = np.nan\n",
    "            \n",
    "        outlier_info[col]['count_after'] = df_clean[col].count()\n",
    "        outlier_info[col]['outliers_removed'] = outlier_info[col]['count_before'] - outlier_info[col]['count_after']\n",
    "        outlier_info[col]['percent_removed'] = (outlier_info[col]['outliers_removed'] / outlier_info[col]['count_before']) * 100\n",
    "        \n",
    "    return df_clean, outlier_info\n",
    "\n",
    "# Detect and remove outliers\n",
    "print(\"\\nDetecting and removing outliers...\")\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "df_clean, outlier_info = detect_and_remove_outliers(df, numerical_columns, method='iqr')\n",
    "\n",
    "# Print outlier removal summary\n",
    "print(\"\\nOutlier removal summary:\")\n",
    "for col, info in outlier_info.items():\n",
    "    if 'outliers_removed' in info:\n",
    "        print(f\"{col}: Removed {info['outliers_removed']} outliers ({info['percent_removed']:.2f}%)\")\n",
    "\n",
    "# 5. Data distribution visualization\n",
    "def plot_distributions(df, features):\n",
    "    \"\"\"Create distribution plots for specified features\"\"\"\n",
    "    n_features = len(features)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_features + 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            # Histogram with KDE\n",
    "            sns.histplot(df[feature].dropna(), kde=True, ax=axes[i], color=colors[i % 10])\n",
    "            axes[i].set_title(f'Distribution of {feature}')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add mean and median lines\n",
    "            mean_val = df[feature].mean()\n",
    "            median_val = df[feature].median()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', \n",
    "                           label=f'Mean: {mean_val:.2f}')\n",
    "            axes[i].axvline(median_val, color='green', linestyle='-.',\n",
    "                           label=f'Median: {median_val:.2f}')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../figures/feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Select key features for distribution analysis\n",
    "key_features = ['PM25', 'AOD', 'T2M', 'BL', 'U10', 'V10']\n",
    "plot_distributions(df_clean, key_features)\n",
    "\n",
    "# 6. Focus on rows with both PM2.5 and AOD data\n",
    "print(\"\\nAnalyzing PM2.5 and AOD relationship...\")\n",
    "df_valid = df_clean.dropna(subset=['PM25', 'AOD']).copy()\n",
    "print(f\"Valid data (no NaN in PM2.5 or AOD): {df_valid.shape}\")\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "corr_coef = df_valid['PM25'].corr(df_valid['AOD'])\n",
    "\n",
    "# Visualize PM2.5 vs AOD scatter plot with correlation\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(df_valid['AOD'], df_valid['PM25'], alpha=0.5, color=colors[0])\n",
    "plt.xlabel('AOD')\n",
    "plt.ylabel('PM2.5 (μg/m³)')\n",
    "plt.title(f'PM2.5 vs AOD (r = {corr_coef:.3f})')\n",
    "plt.grid(True)\n",
    "plt.savefig('../figures/pm25_aod_scatter.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 7. Correlation analysis\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df_valid.corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap=cmap, annot=True, fmt='.2f', \n",
    "           center=0, square=True, linewidths=.5)\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Focus on PM2.5 correlations\n",
    "pm25_corr = corr_matrix['PM25'].sort_values(ascending=False)\n",
    "print(\"\\nFeatures correlation with PM2.5:\")\n",
    "print(pm25_corr)\n",
    "\n",
    "# Create horizontal bar plot of correlations with PM2.5\n",
    "plt.figure(figsize=(10, 8))\n",
    "pm25_corr.drop('PM25').plot(kind='barh', color=[colors[i % 10] for i in range(len(pm25_corr)-1)])\n",
    "plt.title('Feature Correlation with PM2.5')\n",
    "plt.xlabel('Pearson Correlation Coefficient')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/pm25_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 8. Time series analysis (if DATE is available)\n",
    "if isinstance(df_valid.index, pd.DatetimeIndex):\n",
    "    # Resample to monthly averages\n",
    "    monthly_data = df_valid.resample('M').mean()\n",
    "    \n",
    "    # Calculate monthly correlations\n",
    "    monthly_corr = monthly_data['PM25'].corr(monthly_data['AOD'])\n",
    "    \n",
    "    # Plot PM2.5 and AOD time series\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), sharex=True)\n",
    "    \n",
    "    # PM2.5 time series\n",
    "    ax1.plot(monthly_data.index, monthly_data['PM25'], 'o-', color=colors[0], linewidth=2)\n",
    "    ax1.set_ylabel('PM2.5 (μg/m³)')\n",
    "    ax1.set_title(f'Monthly Average PM2.5 Concentration')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # AOD time series\n",
    "    ax2.plot(monthly_data.index, monthly_data['AOD'], 'o-', color=colors[1], linewidth=2)\n",
    "    ax2.set_ylabel('AOD')\n",
    "    ax2.set_title(f'Monthly Average AOD (r = {monthly_corr:.3f})')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/time_series_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract seasonal patterns\n",
    "    monthly_data['Month'] = monthly_data.index.month\n",
    "    seasonal_data = monthly_data.groupby('Month').mean()\n",
    "    \n",
    "    # Calculate seasonal correlations\n",
    "    seasonal_corr = seasonal_data['PM25'].corr(seasonal_data['AOD'])\n",
    "    \n",
    "    # Plot seasonal patterns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    ax1.plot(seasonal_data.index, seasonal_data['PM25'], 'o-', color=colors[0], linewidth=2)\n",
    "    ax1.set_xticks(range(1, 13))\n",
    "    ax1.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    ax1.set_ylabel('PM2.5 (μg/m³)')\n",
    "    ax1.set_title('Seasonal Pattern of PM2.5')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(seasonal_data.index, seasonal_data['AOD'], 'o-', color=colors[1], linewidth=2)\n",
    "    ax2.set_xticks(range(1, 13))\n",
    "    ax2.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    ax2.set_ylabel('AOD')\n",
    "    ax2.set_title(f'Seasonal Pattern of AOD (r = {seasonal_corr:.3f})')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/seasonal_patterns.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 9. Save data for further analysis\n",
    "df_valid.to_csv('../data/cleaned_data.csv')\n",
    "print(\"\\nPart 1 completed. Data saved for further analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PART 2: FEATURE ENGINEERING & PREPROCESSING\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load the cleaned data from Part 1\n",
    "print(\"Loading cleaned data...\")\n",
    "df_cleaned = pd.read_csv('../data/cleaned_data.csv', parse_dates=['date'], index_col='date')\n",
    "print(f\"Cleaned data shape: {df_cleaned.shape}\")\n",
    "\n",
    "# 2. PM2.5/AOD Ratio Analysis (important for research papers)\n",
    "df_cleaned['PM25_AOD_Ratio'] = df_cleaned['PM25'] / df_cleaned['AOD']\n",
    "\n",
    "# Remove infinite values from division by zero\n",
    "df_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Calculate ratio statistics before outlier removal\n",
    "ratio_stats_before = df_cleaned['PM25_AOD_Ratio'].describe()\n",
    "print(\"\\nPM2.5/AOD Ratio Statistics (before outlier removal):\")\n",
    "print(ratio_stats_before)\n",
    "\n",
    "# Outlier removal using Z-score method\n",
    "print(\"\\nRemoving outliers...\")\n",
    "# Function to remove outliers using z-score\n",
    "def remove_outliers(df, columns, threshold=3):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            z_scores = stats.zscore(df_clean[col].dropna())\n",
    "            abs_z_scores = np.abs(z_scores)\n",
    "            filtered_entries = abs_z_scores < threshold\n",
    "            df_clean.loc[df_clean[col].dropna().index[~filtered_entries], col] = np.nan\n",
    "            print(f\"Removed {(~filtered_entries).sum()} outliers from {col}\")\n",
    "    return df_clean\n",
    "\n",
    "# Apply outlier removal to key columns\n",
    "outlier_columns = ['PM25', 'AOD', 'PM25_AOD_Ratio', 'T2M', 'BL', 'SP', 'D2M']\n",
    "outlier_columns = [col for col in outlier_columns if col in df_cleaned.columns]\n",
    "df_cleaned = remove_outliers(df_cleaned, outlier_columns)\n",
    "\n",
    "# Calculate ratio statistics after outlier removal\n",
    "ratio_stats_after = df_cleaned['PM25_AOD_Ratio'].describe()\n",
    "print(\"\\nPM2.5/AOD Ratio Statistics (after outlier removal):\")\n",
    "print(ratio_stats_after)\n",
    "\n",
    "# Visualize the ratio distribution after outlier removal\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(df_cleaned['PM25_AOD_Ratio'].dropna(), kde=True, bins=30)\n",
    "plt.axvline(df_cleaned['PM25_AOD_Ratio'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {df_cleaned[\"PM25_AOD_Ratio\"].mean():.2f}')\n",
    "plt.axvline(df_cleaned['PM25_AOD_Ratio'].median(), color='green', linestyle='-.',\n",
    "           label=f'Median: {df_cleaned[\"PM25_AOD_Ratio\"].median():.2f}')\n",
    "plt.xlabel('PM2.5/AOD Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of PM2.5/AOD Ratio (After Outlier Removal)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig('../figures/pm25_aod_ratio_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Feature Engineering\n",
    "# Add temporal features if DATE is the index\n",
    "if isinstance(df_cleaned.index, pd.DatetimeIndex):\n",
    "    print(\"\\nAdding temporal features...\")\n",
    "    # Extract temporal features\n",
    "    df_cleaned['DayOfYear'] = df_cleaned.index.dayofyear\n",
    "\n",
    "# 4. Analyze PM2.5-AOD relationship\n",
    "# Calculate correlation for the scatter plot\n",
    "overall_corr = df_cleaned['PM25'].corr(df_cleaned['AOD'])\n",
    "\n",
    "# Plot PM2.5 vs AOD\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(df_cleaned['AOD'], df_cleaned['PM25'], alpha=0.6)\n",
    "plt.xlabel('AOD')\n",
    "plt.ylabel('PM2.5 (μg/m³)')\n",
    "plt.title(f'PM2.5 vs AOD Relationship (Overall r = {overall_corr:.3f})')\n",
    "plt.grid(True)\n",
    "plt.savefig('../figures/pm25_aod_relationship.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 5. Interaction features\n",
    "print(\"\\nCreating interaction features...\")\n",
    "# AOD × meteorological interactions\n",
    "df_cleaned['AOD_T2M'] = df_cleaned['AOD'] * df_cleaned['T2M']\n",
    "df_cleaned['AOD_BL'] = df_cleaned['AOD'] * df_cleaned['BL']\n",
    "df_cleaned['AOD_RH'] = df_cleaned['AOD'] * df_cleaned['D2M'] if 'D2M' in df_cleaned.columns else 0\n",
    "\n",
    "# Wind components interaction (for wind speed)\n",
    "if 'U10' in df_cleaned.columns and 'V10' in df_cleaned.columns:\n",
    "    df_cleaned['WindSpeed'] = np.sqrt(df_cleaned['U10']**2 + df_cleaned['V10']**2)\n",
    "    df_cleaned['AOD_WindSpeed'] = df_cleaned['AOD'] * df_cleaned['WindSpeed']\n",
    "\n",
    "# 6. Feature selection\n",
    "# Select features for modeling (adjust based on your dataset)\n",
    "potential_features = ['AOD', 'T2M', 'BL', 'SP', 'D2M', 'U10', 'V10', \n",
    "                     'WindSpeed', 'AOD_T2M', 'AOD_BL']\n",
    "\n",
    "# Filter to only include features that exist in the dataframe\n",
    "features = [f for f in potential_features if f in df_cleaned.columns]\n",
    "target = 'PM25'\n",
    "\n",
    "print(f\"\\nSelected features for modeling: {features}\")\n",
    "\n",
    "# 7. Data split\n",
    "print(\"\\nSplitting data into training and testing sets...\")\n",
    "X = df_cleaned[features].copy()\n",
    "y = df_cleaned[target].copy()\n",
    "\n",
    "# Handle any remaining missing values\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "print(f\"Features shape after handling missing values: {X.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# 8. Feature scaling\n",
    "print(\"\\nScaling features...\")\n",
    "# RobustScaler is good for data with outliers\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for better interpretability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# 9. Save processed data for modeling\n",
    "# Create a dictionary with all necessary data for modeling\n",
    "processed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'features': features,\n",
    "    'scaler': scaler\n",
    "}\n",
    "\n",
    "# Save using numpy (more reliable for objects like the scaler)\n",
    "np.save('../data/processed_data.npy', processed_data, allow_pickle=True)\n",
    "print(\"\\nPart 2 completed. Processed data saved for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PART 3: MODEL DEVELOPMENT & EVALUATION\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, RandomizedSearchCV, learning_curve\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# 1. Load processed data\n",
    "print(\"Loading processed data...\")\n",
    "processed_data = np.load('../data/processed_data.npy', allow_pickle=True).item()\n",
    "\n",
    "X_train = processed_data['X_train']\n",
    "X_test = processed_data['X_test']\n",
    "y_train = processed_data['y_train']\n",
    "y_test = processed_data['y_test']\n",
    "X_train_scaled = processed_data['X_train_scaled']\n",
    "X_test_scaled = processed_data['X_test_scaled']\n",
    "features = processed_data['features']\n",
    "scaler = processed_data['scaler']\n",
    "\n",
    "# Check for NaN values in target variables and handle them\n",
    "print(\"Checking for NaN values in the data...\")\n",
    "if y_train.isna().any():\n",
    "    print(f\"Found {y_train.isna().sum()} NaN values in y_train. Removing rows with NaN values...\")\n",
    "    # Create a mask for non-NaN values in y_train\n",
    "    train_mask = ~y_train.isna()\n",
    "    X_train = X_train[train_mask]\n",
    "    X_train_scaled = X_train_scaled[train_mask]\n",
    "    y_train = y_train[train_mask]\n",
    "    print(f\"After removing NaN values: {len(y_train)} training samples remaining\")\n",
    "\n",
    "if y_test.isna().any():\n",
    "    print(f\"Found {y_test.isna().sum()} NaN values in y_test. Removing rows with NaN values...\")\n",
    "    # Create a mask for non-NaN values in y_test\n",
    "    test_mask = ~y_test.isna()\n",
    "    X_test = X_test[test_mask]\n",
    "    X_test_scaled = X_test_scaled[test_mask]\n",
    "    y_test = y_test[test_mask]\n",
    "    print(f\"After removing NaN values: {len(y_test)} test samples remaining\")\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Features: {features}\")\n",
    "\n",
    "# 2. Define models to evaluate\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1),\n",
    "    'KNN': KNeighborsRegressor(n_neighbors=15)  # Increased n_neighbors to reduce overfitting\n",
    "}\n",
    "\n",
    "# 3. Create AOD-only model for baseline comparison\n",
    "print(\"\\nCreating AOD-only baseline model...\")\n",
    "X_aod_train = X_train[['AOD']].values.reshape(-1, 1)\n",
    "X_aod_test = X_test[['AOD']].values.reshape(-1, 1)\n",
    "\n",
    "aod_model = LinearRegression()\n",
    "aod_model.fit(X_aod_train, y_train)\n",
    "aod_pred = aod_model.predict(X_aod_test)\n",
    "\n",
    "aod_mae = mean_absolute_error(y_test, aod_pred)\n",
    "aod_rmse = np.sqrt(mean_squared_error(y_test, aod_pred))\n",
    "aod_r2 = r2_score(y_test, aod_pred)\n",
    "\n",
    "print(f\"AOD-only model performance:\")\n",
    "print(f\"  MAE: {aod_mae:.2f}\")\n",
    "print(f\"  RMSE: {aod_rmse:.2f}\")\n",
    "print(f\"  R²: {aod_r2:.4f}\")\n",
    "print(f\"  AOD coefficient: {aod_model.coef_[0]:.4f}\")\n",
    "print(f\"  Intercept: {aod_model.intercept_:.4f}\")\n",
    "\n",
    "# 4. Model evaluation function\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"Train and evaluate a model, returning performance metrics\"\"\"\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Return metrics and predictions\n",
    "    return {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'train_mae': train_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_r2': test_r2,\n",
    "        'y_pred': y_test_pred\n",
    "    }\n",
    "\n",
    "# 5. Perform 10-fold cross-validation for all models\n",
    "print(\"\\nPerforming 10-fold cross-validation for all models...\")\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Cross-validating {name}...\")\n",
    "    \n",
    "    # R² scores\n",
    "    r2_scores = cross_val_score(model, X_train_scaled, y_train, \n",
    "                              cv=cv, scoring='r2', n_jobs=-1)\n",
    "    \n",
    "    # RMSE scores (negative converted to positive)\n",
    "    rmse_scores = -cross_val_score(model, X_train_scaled, y_train, \n",
    "                                 cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    # MAE scores (negative converted to positive)\n",
    "    mae_scores = -cross_val_score(model, X_train_scaled, y_train, \n",
    "                                cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'R² (mean)': r2_scores.mean(),\n",
    "        'R² (std)': r2_scores.std(),\n",
    "        'RMSE (mean)': rmse_scores.mean(),\n",
    "        'RMSE (std)': rmse_scores.std(),\n",
    "        'MAE (mean)': mae_scores.mean(),\n",
    "        'MAE (std)': mae_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  Mean R²: {r2_scores.mean():.4f} ± {r2_scores.std():.4f}\")\n",
    "    print(f\"  Mean RMSE: {rmse_scores.mean():.2f} ± {rmse_scores.std():.2f}\")\n",
    "\n",
    "# Create DataFrame for CV results\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "cv_results_df = cv_results_df.sort_values('R² (mean)', ascending=False).reset_index(drop=True)\n",
    "cv_results_df.to_csv('../results/cross_validation_results.csv', index=False)\n",
    "\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(cv_results_df)\n",
    "\n",
    "# 6. Hyperparameter tuning for all models\n",
    "print(\"\\nPerforming hyperparameter optimization for all models...\")\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Linear Regression': None,  # No hyperparameters to tune\n",
    "    \n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'reg_alpha': [0, 0.1, 1.0],  # L1 regularization to prevent overfitting\n",
    "        'reg_lambda': [0, 0.1, 1.0]  # L2 regularization to prevent overfitting\n",
    "    },\n",
    "    \n",
    "    'SVR': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'linear', 'poly'],\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    \n",
    "    'KNN': {\n",
    "        'n_neighbors': [15, 20, 25, 30, 35, 40],  # Increased n_neighbors range to address overfitting\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "        'p': [1, 2, 3],\n",
    "        'leaf_size': [10, 30, 50, 100]  # Added leaf_size parameter for efficiency\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store tuned models and their results\n",
    "tuned_models = {}\n",
    "tuned_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if param_grids[name] is None:\n",
    "        print(f\"Skipping hyperparameter tuning for {name} (no parameters to tune)\")\n",
    "        # Still evaluate the model\n",
    "        result = evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test, name)\n",
    "        tuned_models[name] = model\n",
    "        tuned_results.append(result)\n",
    "        continue\n",
    "    \n",
    "    print(f\"Tuning hyperparameters for {name}...\")\n",
    "    \n",
    "    # Create a fresh instance of the model\n",
    "    model_to_tune = type(model)()\n",
    "    \n",
    "    # Set up randomized search\n",
    "    n_iter = 30  # Number of parameter settings to try\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model_to_tune,\n",
    "        param_distributions=param_grids[name],\n",
    "        n_iter=n_iter,\n",
    "        scoring='r2',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Perform hyperparameter search\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get best parameters\n",
    "    print(f\"Best parameters for {name}:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Get best model\n",
    "    tuned_model = random_search.best_estimator_\n",
    "    tuned_models[name] = tuned_model\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    result = evaluate_model(tuned_model, X_train_scaled, y_train, X_test_scaled, y_test, name)\n",
    "    tuned_results.append(result)\n",
    "    \n",
    "    print(f\"  Test R²: {result['test_r2']:.4f}, RMSE: {result['test_rmse']:.2f}\")\n",
    "\n",
    "# Create DataFrame for tuned model results\n",
    "tuned_results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': r['model_name'],\n",
    "        'Train R²': r['train_r2'],\n",
    "        'Test R²': r['test_r2'],\n",
    "        'Train RMSE': r['train_rmse'],\n",
    "        'Test RMSE': r['test_rmse'],\n",
    "        'Train MAE': r['train_mae'],\n",
    "        'Test MAE': r['test_mae'],\n",
    "        'Improvement over AOD-only (%)': (r['test_r2'] - aod_r2) / aod_r2 * 100 if aod_r2 > 0 else np.inf\n",
    "    } for r in tuned_results\n",
    "])\n",
    "\n",
    "# Sort by test R² score\n",
    "tuned_results_df = tuned_results_df.sort_values('Test R²', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save results for paper\n",
    "tuned_results_df.to_csv('../results/tuned_model_comparison.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nTuned Model Performance Comparison:\")\n",
    "print(tuned_results_df)\n",
    "\n",
    "# 7. Check for overfitting and underfitting\n",
    "print(\"\\nChecking for overfitting and underfitting in tuned models...\")\n",
    "\n",
    "# Calculate the difference between train and test R²\n",
    "tuned_results_df['R² Difference'] = tuned_results_df['Train R²'] - tuned_results_df['Test R²']\n",
    "tuned_results_df['Overfitting Risk'] = tuned_results_df['R² Difference'].apply(\n",
    "    lambda x: 'High' if x > 0.1 else ('Medium' if x > 0.05 else 'Low')\n",
    ")\n",
    "\n",
    "print(\"\\nOverfitting analysis:\")\n",
    "print(tuned_results_df[['Model', 'Train R²', 'Test R²', 'R² Difference', 'Overfitting Risk']])\n",
    "\n",
    "# Plot learning curves for models with high overfitting risk\n",
    "models_to_check = tuned_results_df[tuned_results_df['Overfitting Risk'] != 'Low']['Model'].tolist()\n",
    "\n",
    "if models_to_check:\n",
    "    print(f\"\\nGenerating learning curves for models with potential overfitting: {models_to_check}\")\n",
    "    \n",
    "    for model_name in models_to_check:\n",
    "        model = tuned_models[model_name]\n",
    "        \n",
    "        # Generate learning curve data\n",
    "        train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model, X_train_scaled, y_train, \n",
    "            train_sizes=train_sizes, cv=5, scoring='r2', n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Calculate mean and std for train/test scores\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        test_mean = np.mean(test_scores, axis=1)\n",
    "        test_std = np.std(test_scores, axis=1)\n",
    "        \n",
    "        # Plot learning curve\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(f'Learning Curve: {model_name}')\n",
    "        plt.xlabel('Training Examples')\n",
    "        plt.ylabel('R² Score')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "        plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='orange')\n",
    "        plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "        plt.plot(train_sizes, test_mean, 'o-', color='orange', label='Cross-validation Score')\n",
    "        \n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(f'../figures/learning_curve_{model_name.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Fix overfitting for tree-based models\n",
    "        if model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost']:\n",
    "            print(f\"\\nAdjusting {model_name} to reduce overfitting...\")\n",
    "            \n",
    "            if model_name == 'Random Forest':\n",
    "                # Increase min_samples_leaf and reduce max_depth\n",
    "                new_model = RandomForestRegressor(\n",
    "                    n_estimators=model.n_estimators,\n",
    "                    max_depth=min(model.max_depth, 15) if model.max_depth else 15,\n",
    "                    min_samples_leaf=max(model.min_samples_leaf, 5),\n",
    "                    min_samples_split=max(model.min_samples_split, 5),\n",
    "                    max_features='sqrt',\n",
    "                    random_state=42\n",
    "                )\n",
    "            \n",
    "            elif model_name == 'Gradient Boosting':\n",
    "                # Reduce learning rate and max_depth, increase min_samples_leaf\n",
    "                new_model = GradientBoostingRegressor(\n",
    "                    n_estimators=model.n_estimators,\n",
    "                    learning_rate=min(model.learning_rate, 0.05),\n",
    "                    max_depth=min(model.max_depth, 5),\n",
    "                    min_samples_leaf=max(model.min_samples_leaf, 5),\n",
    "                    min_samples_split=max(model.min_samples_split, 5),\n",
    "                    subsample=0.8,\n",
    "                    random_state=42\n",
    "                )\n",
    "            \n",
    "            elif model_name == 'XGBoost':\n",
    "                # Add regularization and reduce complexity\n",
    "                new_model = XGBRegressor(\n",
    "                    n_estimators=model.n_estimators,\n",
    "                    learning_rate=min(model.learning_rate, 0.05),\n",
    "                    max_depth=min(model.max_depth, 5),\n",
    "                    min_child_weight=max(model.min_child_weight if hasattr(model, 'min_child_weight') else 1, 3),\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    reg_alpha=0.5,  # L1 regularization\n",
    "                    reg_lambda=1.0,  # L2 regularization\n",
    "                    random_state=42\n",
    "                )\n",
    "            \n",
    "            # Evaluate adjusted model\n",
    "            new_result = evaluate_model(new_model, X_train_scaled, y_train, X_test_scaled, y_test, f\"{model_name} (Adjusted)\")\n",
    "            \n",
    "            print(f\"Original {model_name}:\")\n",
    "            print(f\"  Train R²: {tuned_results_df[tuned_results_df['Model'] == model_name]['Train R²'].values[0]:.4f}\")\n",
    "            print(f\"  Test R²: {tuned_results_df[tuned_results_df['Model'] == model_name]['Test R²'].values[0]:.4f}\")\n",
    "            print(f\"  R² Difference: {tuned_results_df[tuned_results_df['Model'] == model_name]['R² Difference'].values[0]:.4f}\")\n",
    "            \n",
    "            print(f\"Adjusted {model_name}:\")\n",
    "            print(f\"  Train R²: {new_result['train_r2']:.4f}\")\n",
    "            print(f\"  Test R²: {new_result['test_r2']:.4f}\")\n",
    "            print(f\"  R² Difference: {new_result['train_r2'] - new_result['test_r2']:.4f}\")\n",
    "            \n",
    "            # If adjusted model performs better on test set or has less overfitting with similar performance\n",
    "            if (new_result['test_r2'] > tuned_results_df[tuned_results_df['Model'] == model_name]['Test R²'].values[0] or\n",
    "                (new_result['train_r2'] - new_result['test_r2'] < tuned_results_df[tuned_results_df['Model'] == model_name]['R² Difference'].values[0] and\n",
    "                 new_result['test_r2'] >= tuned_results_df[tuned_results_df['Model'] == model_name]['Test R²'].values[0] - 0.01)):\n",
    "                \n",
    "                print(f\"Replacing original {model_name} with adjusted version (better generalization)\")\n",
    "                tuned_models[model_name] = new_model\n",
    "                \n",
    "                # Update results\n",
    "                for i, result in enumerate(tuned_results):\n",
    "                    if result['model_name'] == model_name:\n",
    "                        tuned_results[i] = new_result\n",
    "                        break\n",
    "                \n",
    "                # Update DataFrame - Fix the KeyError by mapping column names correctly\n",
    "                mapping = {\n",
    "                    'Train R²': 'train_r2',\n",
    "                    'Test R²': 'test_r2',\n",
    "                    'Train RMSE': 'train_rmse',\n",
    "                    'Test RMSE': 'test_rmse',\n",
    "                    'Train MAE': 'train_mae',\n",
    "                    'Test MAE': 'test_mae'\n",
    "                }\n",
    "                \n",
    "                for df_col, result_key in mapping.items():\n",
    "                    tuned_results_df.loc[tuned_results_df['Model'] == model_name, df_col] = new_result[result_key]\n",
    "                \n",
    "                tuned_results_df.loc[tuned_results_df['Model'] == model_name, 'R² Difference'] = new_result['train_r2'] - new_result['test_r2']\n",
    "                tuned_results_df.loc[tuned_results_df['Model'] == model_name, 'Overfitting Risk'] = 'Low' if (new_result['train_r2'] - new_result['test_r2']) <= 0.05 else 'Medium'\n",
    "            else:\n",
    "                print(f\"Keeping original {model_name} (better overall performance)\")\n",
    "        \n",
    "        # Special handling for KNN which tends to overfit severely\n",
    "        elif model_name == 'KNN':\n",
    "            print(f\"\\nAdjusting KNN to reduce overfitting...\")\n",
    "            \n",
    "            # Create a new KNN model with more neighbors to reduce overfitting\n",
    "            new_model = KNeighborsRegressor(\n",
    "                n_neighbors=max(model.n_neighbors, 40),  # Significantly increase neighbors\n",
    "                weights='uniform',  # Use uniform weights to reduce overfitting\n",
    "                metric='euclidean',\n",
    "                leaf_size=50\n",
    "            )\n",
    "            \n",
    "            # Evaluate adjusted model\n",
    "            new_result = evaluate_model(new_model, X_train_scaled, y_train, X_test_scaled, y_test, \"KNN (Adjusted)\")\n",
    "            \n",
    "            print(f\"Original KNN:\")\n",
    "            print(f\"  Train R²: {tuned_results_df[tuned_results_df['Model'] == 'KNN']['Train R²'].values[0]:.4f}\")\n",
    "            print(f\"  Test R²: {tuned_results_df[tuned_results_df['Model'] == 'KNN']['Test R²'].values[0]:.4f}\")\n",
    "            print(f\"  R² Difference: {tuned_results_df[tuned_results_df['Model'] == 'KNN']['R² Difference'].values[0]:.4f}\")\n",
    "            \n",
    "            print(f\"Adjusted KNN:\")\n",
    "            print(f\"  Train R²: {new_result['train_r2']:.4f}\")\n",
    "            print(f\"  Test R²: {new_result['test_r2']:.4f}\")\n",
    "            print(f\"  R² Difference: {new_result['train_r2'] - new_result['test_r2']:.4f}\")\n",
    "            \n",
    "            # If adjusted model has less overfitting with similar or better test performance\n",
    "            if new_result['train_r2'] - new_result['test_r2'] < tuned_results_df[tuned_results_df['Model'] == 'KNN']['R² Difference'].values[0]:\n",
    "                print(\"Replacing original KNN with adjusted version (better generalization)\")\n",
    "                tuned_models['KNN'] = new_model\n",
    "                \n",
    "                # Update results\n",
    "                for i, result in enumerate(tuned_results):\n",
    "                    if result['model_name'] == 'KNN':\n",
    "                        tuned_results[i] = new_result\n",
    "                        break\n",
    "                \n",
    "                # Update DataFrame\n",
    "                mapping = {\n",
    "                    'Train R²': 'train_r2',\n",
    "                    'Test R²': 'test_r2',\n",
    "                    'Train RMSE': 'train_rmse',\n",
    "                    'Test RMSE': 'test_rmse',\n",
    "                    'Train MAE': 'train_mae',\n",
    "                    'Test MAE': 'test_mae'\n",
    "                }\n",
    "                \n",
    "                for df_col, result_key in mapping.items():\n",
    "                    tuned_results_df.loc[tuned_results_df['Model'] == 'KNN', df_col] = new_result[result_key]\n",
    "                \n",
    "                tuned_results_df.loc[tuned_results_df['Model'] == 'KNN', 'R² Difference'] = new_result['train_r2'] - new_result['test_r2']\n",
    "                tuned_results_df.loc[tuned_results_df['Model'] == 'KNN', 'Overfitting Risk'] = 'Low' if (new_result['train_r2'] - new_result['test_r2']) <= 0.05 else 'Medium'\n",
    "            else:\n",
    "                print(\"Keeping original KNN (better overall performance)\")\n",
    "\n",
    "# Check for underfitting (low R² on both train and test)\n",
    "underfitting_models = tuned_results_df[(tuned_results_df['Train R²'] < 0.5) & (tuned_results_df['Test R²'] < 0.5)]['Model'].tolist()\n",
    "\n",
    "if underfitting_models:\n",
    "    print(f\"\\nModels with potential underfitting: {underfitting_models}\")\n",
    "    print(\"Consider increasing model complexity or feature engineering for these models.\")\n",
    "\n",
    "# 8. Visualize model comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Test R²', y='Model', data=tuned_results_df)\n",
    "plt.axvline(x=aod_r2, color='red', linestyle='--', \n",
    "           label=f'AOD-only baseline (R²={aod_r2:.4f})')\n",
    "plt.title('Tuned Model Comparison: Test R² Score')\n",
    "plt.xlabel('R² Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('../figures/tuned_model_comparison_r2.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Test RMSE', y='Model', data=tuned_results_df)\n",
    "plt.axvline(x=aod_rmse, color='red', linestyle='--', \n",
    "           label=f'AOD-only baseline (RMSE={aod_rmse:.2f})')\n",
    "plt.title('Tuned Model Comparison: Test RMSE')\n",
    "plt.xlabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('../figures/tuned_model_comparison_rmse.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 9. Visualize train vs test performance to identify overfitting\n",
    "plt.figure(figsize=(14, 8))\n",
    "models_df = tuned_results_df.copy()\n",
    "models_df = pd.melt(models_df, \n",
    "                   id_vars=['Model'], \n",
    "                   value_vars=['Train R²', 'Test R²'],\n",
    "                   var_name='Dataset', value_name='R² Score')\n",
    "\n",
    "sns.barplot(x='Model', y='R² Score', hue='Dataset', data=models_df)\n",
    "plt.title('Train vs Test R² Score Comparison')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/train_vs_test_r2.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 10. Select the best model based on test R²\n",
    "best_result = tuned_results[tuned_results_df.iloc[0].name]\n",
    "best_model = best_result['model']\n",
    "best_model_name = best_result['model_name']\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Test R²: {best_result['test_r2']:.4f}\")\n",
    "print(f\"Test RMSE: {best_result['test_rmse']:.2f}\")\n",
    "print(f\"Improvement over AOD-only: {(best_result['test_r2'] - aod_r2) / aod_r2 * 100:.2f}%\")\n",
    "print(f\"Overfitting risk: {tuned_results_df[tuned_results_df['Model'] == best_model_name]['Overfitting Risk'].values[0]}\")\n",
    "\n",
    "# 11. Visualize actual vs. predicted for best model\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(y_test, best_result['y_pred'], alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "\n",
    "# Calculate correlation coefficient and add to plot\n",
    "correlation = np.corrcoef(y_test, best_result['y_pred'])[0, 1]\n",
    "plt.annotate(f'R = {correlation:.4f}', \n",
    "             xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "             fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.xlabel('Actual PM2.5 (μg/m³)')\n",
    "plt.ylabel('Predicted PM2.5 (μg/m³)')\n",
    "plt.title(f'Actual vs. Predicted PM2.5 Using {best_model_name}')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 12. Residual analysis\n",
    "residuals = y_test - best_result['y_pred']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(best_result['y_pred'], residuals, alpha=0.7)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted PM2.5 (μg/m³)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot for Best Model')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/residual_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.xlabel('Residual Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/residual_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 13. Save best model for further analysis\n",
    "model_info = {\n",
    "    'model': best_model,\n",
    "    'model_name': best_model_name,\n",
    "    'features': features,\n",
    "    'scaler': scaler,\n",
    "    'performance': {\n",
    "        'r2': best_result['test_r2'],\n",
    "        'rmse': best_result['test_rmse'],\n",
    "        'mae': best_result['test_mae']\n",
    "    },\n",
    "    'aod_only_performance': {\n",
    "        'r2': aod_r2,\n",
    "        'rmse': aod_rmse,\n",
    "        'mae': aod_mae\n",
    "    },\n",
    "    'aod_model': aod_model\n",
    "}\n",
    "\n",
    "np.save('../models/best_model.npy', model_info, allow_pickle=True)\n",
    "print(\"\\nPart 3 completed. Best model saved for further analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PART 4: FEATURE IMPORTANCE & ADVANCED ANALYSIS\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance, partial_dependence, PartialDependenceDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "import warnings\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style for publication quality\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# 1. Load best model and data\n",
    "print(\"Loading best model and data...\")\n",
    "model_info = np.load('../models/best_model.npy', allow_pickle=True).item()\n",
    "best_model = model_info['model']\n",
    "best_model_name = model_info['model_name']\n",
    "features = model_info['features']\n",
    "scaler = model_info['scaler']\n",
    "\n",
    "processed_data = np.load('../data/processed_data.npy', allow_pickle=True).item()\n",
    "X_train = processed_data['X_train']\n",
    "X_test = processed_data['X_test']\n",
    "y_train = processed_data['y_train']\n",
    "y_test = processed_data['y_test']\n",
    "X_train_scaled = processed_data['X_train_scaled']\n",
    "X_test_scaled = processed_data['X_test_scaled']\n",
    "\n",
    "# Check for NaN values and handle them\n",
    "print(\"Checking for NaN values in the data...\")\n",
    "if X_test_scaled.isna().any().any() or y_test.isna().any():\n",
    "    print(\"Found NaN values in test data. Removing rows with NaN values...\")\n",
    "    # Create a mask for non-NaN values\n",
    "    mask = ~np.isnan(y_test) & ~np.isnan(X_test_scaled).any(axis=1)\n",
    "    X_test_scaled = X_test_scaled[mask]\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    print(f\"After removing NaN values: {len(y_test)} samples remaining\")\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Performance: R²={model_info['performance']['r2']:.4f}, RMSE={model_info['performance']['rmse']:.2f}\")\n",
    "\n",
    "# 2. Feature importance analysis based on model type\n",
    "importances = None\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # For tree-based models that have feature_importances_ attribute\n",
    "    print(\"\\nExtracting built-in feature importances...\")\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Create dataframe of feature importances\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Save for paper\n",
    "    importance_df.to_csv('../results/feature_importances.csv', index=False)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title(f'Feature Importance from {best_model_name}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_importances.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top features\n",
    "    print(\"\\nTop 5 important features:\")\n",
    "    print(importance_df.head(5))\n",
    "    \n",
    "    # Find AOD importance and rank\n",
    "    aod_importance = importance_df[importance_df['Feature'] == 'AOD']\n",
    "    if not aod_importance.empty:\n",
    "        aod_rank = importance_df[importance_df['Feature'] == 'AOD'].index[0] + 1\n",
    "        print(f\"\\nAOD importance: {aod_importance['Importance'].values[0]:.4f}\")\n",
    "        print(f\"AOD rank: {aod_rank} out of {len(features)}\")\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models that have coef_ attribute\n",
    "    print(\"\\nExtracting coefficients from linear model...\")\n",
    "    if len(best_model.coef_.shape) == 1:\n",
    "        coeffs = best_model.coef_\n",
    "    else:\n",
    "        coeffs = best_model.coef_[0]\n",
    "    \n",
    "    # Create dataframe of coefficients\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Coefficient': coeffs,\n",
    "        'Abs_Coefficient': np.abs(coeffs)\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute coefficient value\n",
    "    coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    # Save for paper\n",
    "    coef_df.to_csv('../results/model_coefficients.csv', index=False)\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['g' if c > 0 else 'r' for c in coef_df['Coefficient']]\n",
    "    plt.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors)\n",
    "    plt.axvline(x=0, color='k', linestyle='--')\n",
    "    plt.title('Model Coefficients (Green=Positive, Red=Negative)')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/model_coefficients.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top coefficients\n",
    "    print(\"\\nTop 5 features by coefficient magnitude:\")\n",
    "    print(coef_df.head(5))\n",
    "    \n",
    "    # Find AOD coefficient and rank\n",
    "    aod_coef = coef_df[coef_df['Feature'] == 'AOD']\n",
    "    if not aod_coef.empty:\n",
    "        aod_rank = coef_df[coef_df['Feature'] == 'AOD'].index[0] + 1\n",
    "        print(f\"\\nAOD coefficient: {aod_coef['Coefficient'].values[0]:.4f}\")\n",
    "        print(f\"AOD rank: {aod_rank} out of {len(features)}\")\n",
    "\n",
    "# 3. Permutation importance (model-agnostic approach)\n",
    "print(\"\\nCalculating permutation importance...\")\n",
    "try:\n",
    "    perm_importance = permutation_importance(\n",
    "        best_model, X_test_scaled, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Create dataframe of permutation importances\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': perm_importance.importances_mean,\n",
    "        'Std': perm_importance.importances_std\n",
    "    })\n",
    "\n",
    "    # Sort by importance\n",
    "    perm_importance_df = perm_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "    # Save for paper\n",
    "    perm_importance_df.to_csv('../results/permutation_importance.csv', index=False)\n",
    "\n",
    "    # Plot permutation importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=perm_importance_df, \n",
    "               xerr=perm_importance_df['Std'])\n",
    "    plt.title('Permutation Feature Importance (Test Set)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/permutation_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print top features by permutation importance\n",
    "    print(\"\\nTop 5 features by permutation importance:\")\n",
    "    print(perm_importance_df.head(5))\n",
    "\n",
    "    # Find AOD permutation importance and rank\n",
    "    aod_perm = perm_importance_df[perm_importance_df['Feature'] == 'AOD']\n",
    "    if not aod_perm.empty:\n",
    "        aod_perm_rank = perm_importance_df[perm_importance_df['Feature'] == 'AOD'].index[0] + 1\n",
    "        print(f\"\\nAOD permutation importance: {aod_perm['Importance'].values[0]:.4f}\")\n",
    "        print(f\"AOD permutation rank: {aod_perm_rank} out of {len(features)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating permutation importance: {str(e)}\")\n",
    "    print(\"Creating empty permutation importance dataframe for downstream code\")\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': np.zeros(len(features)),\n",
    "        'Std': np.zeros(len(features))\n",
    "    })\n",
    "    perm_importance_df = perm_importance_df.sort_values('Importance', ascending=False)\n",
    "    aod_perm_rank = 0\n",
    "\n",
    "# 4. SHAP values for detailed feature impact (if model supports it)\n",
    "try:\n",
    "    print(\"\\nCalculating SHAP values for feature interpretation...\")\n",
    "    # Create explainer\n",
    "    explainer = shap.Explainer(best_model, X_train_scaled)\n",
    "    \n",
    "    # Calculate SHAP values on test set\n",
    "    shap_values = explainer(X_test_scaled)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, X_test_scaled, plot_type=\"bar\", show=False)\n",
    "    plt.title('Feature Impact (SHAP Values)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed dependency plot for AOD\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.dependence_plot(\"AOD\", shap_values.values, X_test_scaled, show=False)\n",
    "    plt.title('SHAP Dependence Plot for AOD')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/shap_aod_dependence.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save SHAP values for paper\n",
    "    shap_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'SHAP_abs_mean': np.abs(shap_values.values).mean(axis=0)\n",
    "    })\n",
    "    shap_df = shap_df.sort_values('SHAP_abs_mean', ascending=False)\n",
    "    shap_df.to_csv('../results/shap_importance.csv', index=False)\n",
    "    \n",
    "    print(\"SHAP analysis completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute SHAP values: {str(e)}\")\n",
    "\n",
    "# 5. Partial Dependence Plots for key features\n",
    "print(\"\\nGenerating Partial Dependence Plots...\")\n",
    "try:\n",
    "    # Top 3 features from previous importance analysis\n",
    "    if 'importance_df' in locals():\n",
    "        top_features = importance_df.head(3)['Feature'].tolist()\n",
    "    elif 'coef_df' in locals():\n",
    "        top_features = coef_df.head(3)['Feature'].tolist()\n",
    "    else:\n",
    "        top_features = perm_importance_df.head(3)['Feature'].tolist()\n",
    "\n",
    "    # Ensure AOD is included in the top features\n",
    "    if 'AOD' not in top_features:\n",
    "        top_features.append('AOD')\n",
    "\n",
    "    # Get feature indices\n",
    "    feature_indices = [list(features).index(feat) for feat in top_features]\n",
    "\n",
    "    # Generate PDPs\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, X_test_scaled, feature_indices, \n",
    "        feature_names=top_features, ax=ax\n",
    "    )\n",
    "    plt.suptitle('Partial Dependence Plots for Top Features', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/partial_dependence_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error generating Partial Dependence Plots: {str(e)}\")\n",
    "\n",
    "# 6. AOD-PM2.5 relationship analysis\n",
    "print(\"\\nAnalyzing AOD-PM2.5 relationship in detail...\")\n",
    "try:\n",
    "    # Create AOD-only model (from loaded model info)\n",
    "    aod_model = model_info['aod_model']\n",
    "    aod_model_performance = model_info['aod_only_performance']\n",
    "\n",
    "    # Compare full model vs. AOD-only model\n",
    "    improvement = (model_info['performance']['r2'] - aod_model_performance['r2']) / aod_model_performance['r2'] * 100\n",
    "\n",
    "    print(f\"AOD-only model: R²={aod_model_performance['r2']:.4f}, RMSE={aod_model_performance['rmse']:.2f}\")\n",
    "    print(f\"Full model: R²={model_info['performance']['r2']:.4f}, RMSE={model_info['performance']['rmse']:.2f}\")\n",
    "    print(f\"Improvement with additional features: {improvement:.2f}%\")\n",
    "\n",
    "    # Visualize AOD-only vs full model predictions\n",
    "    aod_preds = aod_model.predict(X_test[['AOD']].values.reshape(-1, 1))\n",
    "    full_preds = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate correlations\n",
    "    aod_only_pearson, _ = pearsonr(y_test, aod_preds)\n",
    "    full_model_pearson, _ = pearsonr(y_test, full_preds)\n",
    "    aod_only_spearman, _ = spearmanr(y_test, aod_preds)\n",
    "    full_model_spearman, _ = spearmanr(y_test, full_preds)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(y_test, aod_preds, alpha=0.5, label=f'AOD-only model (r={aod_only_pearson:.3f}, ρ={aod_only_spearman:.3f})', color='blue')\n",
    "    plt.scatter(y_test, full_preds, alpha=0.5, label=f'{best_model_name} (r={full_model_pearson:.3f}, ρ={full_model_spearman:.3f})', color='red')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', label='1:1 line')\n",
    "    plt.xlabel('Actual PM2.5 (μg/m³)')\n",
    "    plt.ylabel('Predicted PM2.5 (μg/m³)')\n",
    "    plt.title('AOD-only vs. Full Model Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/aod_vs_full_model.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in AOD-PM2.5 relationship analysis: {str(e)}\")\n",
    "    aod_only_pearson, full_model_pearson = 0, 0\n",
    "    aod_only_spearman, full_model_spearman = 0, 0\n",
    "\n",
    "# 7. PM2.5/AOD ratio analysis\n",
    "print(\"\\nAnalyzing PM2.5/AOD ratio...\")\n",
    "try:\n",
    "    # Reload cleaned data to get the ratio\n",
    "    df_cleaned = pd.read_csv('../data/cleaned_data.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "    # Calculate PM2.5/AOD ratio\n",
    "    df_cleaned['PM25_AOD_Ratio'] = df_cleaned['PM25'] / df_cleaned['AOD']\n",
    "    df_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Basic statistics of the ratio\n",
    "    ratio_stats = df_cleaned['PM25_AOD_Ratio'].describe()\n",
    "    print(\"PM2.5/AOD Ratio Statistics:\")\n",
    "    print(ratio_stats)\n",
    "\n",
    "    # Save ratio statistics for paper\n",
    "    ratio_stats.to_csv('../results/pm25_aod_ratio_stats.csv')\n",
    "\n",
    "    # Get best result from previous analysis\n",
    "    try:\n",
    "        best_result = np.load('../results/best_model_results.npy', allow_pickle=True).item()\n",
    "    except:\n",
    "        # If best_result is not available, create a placeholder\n",
    "        print(\"Warning: best_result not found. Creating placeholder for downstream code.\")\n",
    "        best_result = {'y_pred': full_preds}\n",
    "\n",
    "    # Add predicted PM2.5 to test data\n",
    "    X_test_reset = X_test.reset_index(drop=True)\n",
    "    test_data = pd.DataFrame({\n",
    "        'Actual_PM25': y_test.reset_index(drop=True),\n",
    "        'Predicted_PM25': best_result['y_pred'],\n",
    "        'AOD': X_test_reset['AOD'],\n",
    "    })\n",
    "    \n",
    "    # Calculate ratios\n",
    "    test_data['Actual_Ratio'] = test_data['Actual_PM25'] / test_data['AOD']\n",
    "    test_data['Predicted_Ratio'] = test_data['Predicted_PM25'] / test_data['AOD']\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Calculate error in ratio prediction\n",
    "    test_data['Ratio_Error'] = test_data['Predicted_Ratio'] - test_data['Actual_Ratio']\n",
    "    test_data['Ratio_Pct_Error'] = (test_data['Ratio_Error'] / test_data['Actual_Ratio']) * 100\n",
    "\n",
    "    # Calculate correlation for ratio prediction\n",
    "    ratio_pearson, _ = pearsonr(test_data['Actual_Ratio'].dropna(), test_data['Predicted_Ratio'].dropna())\n",
    "    ratio_spearman, _ = spearmanr(test_data['Actual_Ratio'].dropna(), test_data['Predicted_Ratio'].dropna())\n",
    "\n",
    "    # Visualize actual vs predicted ratio\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(test_data['Actual_Ratio'], test_data['Predicted_Ratio'], alpha=0.7)\n",
    "    plt.plot([test_data['Actual_Ratio'].min(), test_data['Actual_Ratio'].max()], \n",
    "             [test_data['Actual_Ratio'].min(), test_data['Actual_Ratio'].max()], 'r--')\n",
    "    plt.xlabel('Actual PM2.5/AOD Ratio')\n",
    "    plt.ylabel('Predicted PM2.5/AOD Ratio')\n",
    "    plt.title(f'Actual vs. Predicted PM2.5/AOD Ratio (r={ratio_pearson:.3f}, ρ={ratio_spearman:.3f})')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/actual_vs_predicted_ratio.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in PM2.5/AOD ratio analysis: {str(e)}\")\n",
    "    ratio_stats = pd.Series({'mean': 0, '50%': 0, 'std': 0})\n",
    "    ratio_pearson, ratio_spearman = 0, 0\n",
    "\n",
    "# 8. Model performance by AOD range\n",
    "print(\"\\nAnalyzing model performance by AOD range...\")\n",
    "try:\n",
    "    # Create AOD bins\n",
    "    aod_bins = pd.cut(X_test['AOD'], bins=5)\n",
    "    X_test_reset = X_test.reset_index(drop=True)\n",
    "    y_test_reset = y_test.reset_index(drop=True)\n",
    "\n",
    "    # Analysis by AOD bin\n",
    "    aod_analysis = pd.DataFrame({\n",
    "        'AOD': X_test_reset['AOD'],\n",
    "        'Actual_PM25': y_test_reset,\n",
    "        'Predicted_PM25': best_result['y_pred'],\n",
    "        'Error': y_test_reset - best_result['y_pred'],\n",
    "        'Pct_Error': (y_test_reset - best_result['y_pred']) / y_test_reset * 100,\n",
    "        'AOD_bin': pd.cut(X_test_reset['AOD'], bins=5)\n",
    "    })\n",
    "\n",
    "    # Group by AOD bin\n",
    "    aod_bin_analysis = aod_analysis.groupby('AOD_bin').agg({\n",
    "        'AOD': ['count', 'mean'],\n",
    "        'Actual_PM25': ['mean'],\n",
    "        'Predicted_PM25': ['mean'],\n",
    "        'Error': ['mean', 'std'],\n",
    "        'Pct_Error': ['mean', 'std']\n",
    "    })\n",
    "\n",
    "    # Save for paper\n",
    "    aod_bin_analysis.to_csv('../results/performance_by_aod_range.csv')\n",
    "\n",
    "    print(\"Performance by AOD range:\")\n",
    "    print(aod_bin_analysis)\n",
    "\n",
    "    # Visualize error by AOD bin\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='AOD_bin', y='Error', data=aod_analysis)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('AOD Range')\n",
    "    plt.ylabel('Prediction Error (μg/m³)')\n",
    "    plt.title('Model Error by AOD Range')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/error_by_aod_range.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in model performance by AOD range analysis: {str(e)}\")\n",
    "\n",
    "# 9. Error Analysis: Where does the model fail?\n",
    "print(\"\\nIdentifying conditions where the model performs poorly...\")\n",
    "try:\n",
    "    # Calculate absolute percentage error\n",
    "    aod_analysis['Abs_Pct_Error'] = np.abs(aod_analysis['Pct_Error'])\n",
    "\n",
    "    # Identify worst predictions (top 5% errors)\n",
    "    threshold = np.percentile(aod_analysis['Abs_Pct_Error'].dropna(), 95)\n",
    "    worst_predictions = aod_analysis[aod_analysis['Abs_Pct_Error'] > threshold]\n",
    "\n",
    "    print(f\"Analyzing {len(worst_predictions)} worst predictions (>95th percentile of error)\")\n",
    "    print(f\"Error threshold: {threshold:.2f}% absolute percentage error\")\n",
    "\n",
    "    # Check if these errors are associated with particular AOD ranges\n",
    "    print(\"\\nAOD distribution in worst predictions:\")\n",
    "    print(worst_predictions['AOD'].describe())\n",
    "\n",
    "    print(\"\\nOverall AOD distribution:\")\n",
    "    print(aod_analysis['AOD'].describe())\n",
    "\n",
    "    # Calculate correlation between AOD and actual PM2.5\n",
    "    aod_pm25_pearson, _ = pearsonr(aod_analysis['AOD'], aod_analysis['Actual_PM25'])\n",
    "    aod_pm25_spearman, _ = spearmanr(aod_analysis['AOD'], aod_analysis['Actual_PM25'])\n",
    "\n",
    "    # Create scatter plot with worst predictions highlighted\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(aod_analysis['AOD'], aod_analysis['Actual_PM25'], alpha=0.5, \n",
    "               label='All predictions', color='blue')\n",
    "    plt.scatter(worst_predictions['AOD'], worst_predictions['Actual_PM25'], alpha=0.8,\n",
    "               label='Worst predictions', color='red')\n",
    "    plt.xlabel('AOD')\n",
    "    plt.ylabel('PM2.5 (μg/m³)')\n",
    "    plt.title(f'Identifying Poor Model Performance Regions (AOD-PM2.5: r={aod_pm25_pearson:.3f}, ρ={aod_pm25_spearman:.3f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/worst_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in model failure analysis: {str(e)}\")\n",
    "    aod_pm25_pearson, aod_pm25_spearman = 0, 0\n",
    "\n",
    "# 10. Generate summary table for paper\n",
    "try:\n",
    "    # Load cross-validation results if available\n",
    "    try:\n",
    "        cv_results_df = pd.read_csv('../results/cross_validation_results.csv')\n",
    "    except:\n",
    "        print(\"Warning: Cross-validation results not found. Using placeholder values.\")\n",
    "        cv_results_df = pd.DataFrame({\n",
    "            'Model': [best_model_name],\n",
    "            'R² (mean)': [model_info['performance']['r2']],\n",
    "            'R² (std)': [0],\n",
    "            'RMSE (mean)': [model_info['performance']['rmse']],\n",
    "            'RMSE (std)': [0]\n",
    "        })\n",
    "    \n",
    "    summary_table = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Best Model', \n",
    "            'R² (Full Model)', \n",
    "            'RMSE (Full Model)', \n",
    "            'MAE (Full Model)',\n",
    "            'R² (AOD-only Model)',\n",
    "            'RMSE (AOD-only Model)',\n",
    "            'Improvement from Meteorological Variables',\n",
    "            'PM2.5/AOD Ratio (Mean)',\n",
    "            'PM2.5/AOD Ratio (Median)',\n",
    "            'PM2.5/AOD Ratio (Std)',\n",
    "            'Top Feature',\n",
    "            'AOD Importance Rank',\n",
    "            'Sample Size',\n",
    "            'R² from Cross-Validation',\n",
    "            'RMSE from Cross-Validation',\n",
    "            'Pearson Correlation (Full Model)',\n",
    "            'Spearman Correlation (Full Model)',\n",
    "            'Pearson Correlation (AOD-only Model)',\n",
    "            'Spearman Correlation (AOD-only Model)'\n",
    "        ],\n",
    "        'Value': [\n",
    "            best_model_name,\n",
    "            f\"{model_info['performance']['r2']:.4f}\",\n",
    "            f\"{model_info['performance']['rmse']:.2f} μg/m³\",\n",
    "            f\"{model_info['performance']['mae']:.2f} μg/m³\",\n",
    "            f\"{model_info['aod_only_performance']['r2']:.4f}\",\n",
    "            f\"{model_info['aod_only_performance']['rmse']:.2f} μg/m³\",\n",
    "            f\"{improvement:.2f}%\",\n",
    "            f\"{ratio_stats['mean']:.2f}\",\n",
    "            f\"{ratio_stats['50%']:.2f}\",\n",
    "            f\"{ratio_stats['std']:.2f}\",\n",
    "            perm_importance_df.iloc[0]['Feature'] if len(perm_importance_df) > 0 else \"N/A\",\n",
    "            f\"{aod_perm_rank} of {len(features)}\" if aod_perm_rank > 0 else \"N/A\",\n",
    "            f\"{len(X_train) + len(X_test)}\",\n",
    "            f\"{cv_results_df.iloc[0]['R² (mean)']:.4f} ± {cv_results_df.iloc[0]['R² (std)']:.4f}\",\n",
    "            f\"{cv_results_df.iloc[0]['RMSE (mean)']:.2f} ± {cv_results_df.iloc[0]['RMSE (std)']:.2f}\",\n",
    "            f\"{full_model_pearson:.4f}\",\n",
    "            f\"{full_model_spearman:.4f}\",\n",
    "            f\"{aod_only_pearson:.4f}\",\n",
    "            f\"{aod_only_spearman:.4f}\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Save summary for paper\n",
    "    summary_table.to_csv('../results/analysis_summary.csv', index=False)\n",
    "\n",
    "    print(\"\\nAnalysis summary:\")\n",
    "    print(summary_table)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating summary table: {str(e)}\")\n",
    "\n",
    "print(\"\\nPart 4 completed. Comprehensive analysis results saved for paper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Part 5: Advanced Visualization and Interpretation\n",
    "#############################################################\n",
    "\n",
    "print(\"## 5. Advanced Visualization and Interpretation\")\n",
    "print(\"Creating advanced visualizations to understand PM2.5 and AOD relationships...\")\n",
    "\n",
    "# Import additional libraries if needed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create figures directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists('../figures'):\n",
    "    os.makedirs('../figures')\n",
    "    print(\"Created '../figures' directory for saving visualizations\")\n",
    "\n",
    "# 5.1 Basic visualizations of PM2.5 vs AOD relationship\n",
    "\n",
    "# Scatterplot with regression line\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.regplot(x='AOD', y='PM25', data=df, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "plt.title('Relationship between PM2.5 and AOD', fontsize=16)\n",
    "plt.xlabel('AOD', fontsize=14)\n",
    "plt.ylabel('PM2.5 (μg/m³)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate correlation\n",
    "corr = df['AOD'].corr(df['PM25'])\n",
    "r2 = corr**2\n",
    "plt.annotate(f\"Correlation: {corr:.3f}\\nR²: {r2:.3f}\", \n",
    "             xy=(0.05, 0.92), xycoords='axes fraction',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "             fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/pm25_aod_relationship.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Created PM2.5 vs AOD relationship plot\")\n",
    "\n",
    "# 5.2 Distribution of PM2.5 and AOD\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PM2.5 distribution\n",
    "sns.histplot(df['PM25'], kde=True, ax=ax1, color='skyblue')\n",
    "ax1.set_title('Distribution of PM2.5 Concentrations', fontsize=14)\n",
    "ax1.set_xlabel('PM2.5 (μg/m³)', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add summary statistics\n",
    "pm_mean = df['PM25'].mean()\n",
    "pm_median = df['PM25'].median()\n",
    "pm_std = df['PM25'].std()\n",
    "ax1.axvline(pm_mean, color='red', linestyle='--', label=f'Mean: {pm_mean:.2f}')\n",
    "ax1.axvline(pm_median, color='green', linestyle='-.', label=f'Median: {pm_median:.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# AOD distribution\n",
    "sns.histplot(df['AOD'], kde=True, ax=ax2, color='lightgreen')\n",
    "ax2.set_title('Distribution of AOD Values', fontsize=14)\n",
    "ax2.set_xlabel('AOD', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add summary statistics\n",
    "aod_mean = df['AOD'].mean()\n",
    "aod_median = df['AOD'].median()\n",
    "aod_std = df['AOD'].std()\n",
    "ax2.axvline(aod_mean, color='red', linestyle='--', label=f'Mean: {aod_mean:.2f}')\n",
    "ax2.axvline(aod_median, color='green', linestyle='-.', label=f'Median: {aod_median:.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/pm25_aod_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Created distribution plots for PM2.5 and AOD\")\n",
    "\n",
    "# 5.4 Meteorological factors influence\n",
    "# Select meteorological features\n",
    "met_features = [col for col in df.columns if col in \n",
    "                ['T2M', 'SP', 'U10', 'V10', 'BL', 'D2M']]\n",
    "\n",
    "if len(met_features) > 0:\n",
    "    # Correlation matrix\n",
    "    corr_columns = ['PM25', 'AOD'] + met_features\n",
    "    corr_matrix = df[corr_columns].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', \n",
    "              linewidths=0.5, vmin=-1, vmax=1, center=0)\n",
    "    plt.title('Correlation Matrix: PM2.5, AOD, and Meteorological Factors', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Created correlation matrix heatmap\")\n",
    "    \n",
    "    # 3D plot with most correlated meteorological factor\n",
    "    if len(met_features) >= 1:\n",
    "        # Find most correlated meteorological feature with PM2.5\n",
    "        pm_corr = abs(corr_matrix['PM25'][met_features]).sort_values(ascending=False)\n",
    "        top_met_feature = pm_corr.index[0]\n",
    "        \n",
    "        # Create 3D plot\n",
    "        fig = plt.figure(figsize=(12, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        scatter = ax.scatter(df['AOD'], df[top_met_feature], df['PM25'],\n",
    "                           c=df[top_met_feature], cmap='viridis', \n",
    "                           s=30, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('AOD', fontsize=12)\n",
    "        ax.set_ylabel(top_met_feature, fontsize=12)\n",
    "        ax.set_zlabel('PM2.5 (μg/m³)', fontsize=12)\n",
    "        ax.set_title(f'3D Relationship: PM2.5, AOD, and {top_met_feature}', fontsize=14)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax, shrink=0.7)\n",
    "        cbar.set_label(top_met_feature, fontsize=12)\n",
    "        \n",
    "        # Rotate the plot for better viewing\n",
    "        ax.view_init(elev=20, azim=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../figures/3d_relationship.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"✓ Created 3D visualization with {top_met_feature}\")\n",
    "else:\n",
    "    print(\"No meteorological features found - skipping meteorological analysis\")\n",
    "\n",
    "# 5.5 Model performance visualization\n",
    "if 'y_test' in locals() and 'y_pred' in locals():\n",
    "    # Check if arrays are compatible for plotting\n",
    "    if hasattr(y_test, 'values'):\n",
    "        y_test_values = y_test.values\n",
    "    else:\n",
    "        y_test_values = y_test\n",
    "        \n",
    "    # Ensure shapes match\n",
    "    if len(y_test_values) != len(y_pred):\n",
    "        print(f\"Warning: y_test ({len(y_test_values)}) and y_pred ({len(y_pred)}) have different lengths.\")\n",
    "        # Use the minimum length\n",
    "        min_len = min(len(y_test_values), len(y_pred))\n",
    "        y_test_values = y_test_values[:min_len]\n",
    "        y_pred = y_pred[:min_len]\n",
    "        print(f\"Using first {min_len} elements for visualization\")\n",
    "        \n",
    "    # Reshape if needed\n",
    "    y_test_values = y_test_values.flatten() if hasattr(y_test_values, 'flatten') else y_test_values\n",
    "    y_pred_flat = y_pred.flatten() if hasattr(y_pred, 'flatten') else y_pred\n",
    "    \n",
    "    # Create visualization of actual vs predicted values\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(y_test_values, y_pred_flat, alpha=0.5)\n",
    "    \n",
    "    # Get min and max for the reference line\n",
    "    min_val = min(y_test_values.min(), y_pred_flat.min())\n",
    "    max_val = max(y_test_values.max(), y_pred_flat.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    plt.xlabel('Actual PM2.5 (μg/m³)', fontsize=12)\n",
    "    plt.ylabel('Predicted PM2.5 (μg/m³)', fontsize=12)\n",
    "    plt.title('Actual vs Predicted PM2.5 Concentrations', fontsize=14)\n",
    "    \n",
    "    # Add performance metrics\n",
    "    if 'all_model_results' in locals():\n",
    "        # Find best model\n",
    "        best_model_name = max(all_model_results, key=lambda x: x['test_r2'])['model_name']\n",
    "        best_test_r2 = max(all_model_results, key=lambda x: x['test_r2'])['test_r2']\n",
    "        best_test_rmse = max(all_model_results, key=lambda x: x['test_r2'])['test_rmse']\n",
    "        best_test_mae = max(all_model_results, key=lambda x: x['test_r2'])['test_mae']\n",
    "        \n",
    "        plt.annotate(f\"Model: {best_model_name}\\nR²: {best_test_r2:.3f}\\nRMSE: {best_test_rmse:.3f}\\nMAE: {best_test_mae:.3f}\", \n",
    "                   xy=(0.05, 0.92), xycoords='axes fraction',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                   fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Created actual vs predicted values plot\")\n",
    "    \n",
    "    # Residual analysis\n",
    "    residuals = y_test_values - y_pred_flat\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Residual plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_pred_flat, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted PM2.5 (μg/m³)', fontsize=12)\n",
    "    plt.ylabel('Residuals', fontsize=12)\n",
    "    plt.title('Residual Plot', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(residuals, kde=True, color='skyblue')\n",
    "    plt.xlabel('Residual Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title('Distribution of Residuals', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add residual statistics\n",
    "    plt.annotate(f\"Mean: {np.mean(residuals):.3f}\\nStd Dev: {np.std(residuals):.3f}\", \n",
    "               xy=(0.05, 0.92), xycoords='axes fraction',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "               fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/residual_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Created residual analysis plots\")\n",
    "    \n",
    "    # Model performance comparison if available\n",
    "    if 'all_model_results' in locals():\n",
    "        # Filter out Ridge, Lasso, and ElasticNet models\n",
    "        filtered_models = [m for m in all_model_results if not any(x in m['model_name'] for x in ['Ridge', 'Lasso', 'ElasticNet'])]\n",
    "        \n",
    "        # Sort models by test R² score\n",
    "        sorted_models = sorted(filtered_models, key=lambda x: x['test_r2'], reverse=True)\n",
    "        model_names = [m['model_name'] for m in sorted_models]\n",
    "        test_r2 = [m['test_r2'] for m in sorted_models]\n",
    "        test_rmse = [m['test_rmse'] for m in sorted_models]\n",
    "        test_mae = [m['test_mae'] for m in sorted_models]\n",
    "        \n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # R² scores\n",
    "        plt.subplot(3, 1, 1)\n",
    "        bars = plt.bar(model_names, test_r2, color='skyblue')\n",
    "        bars[0].set_color('darkblue')  # Highlight best model\n",
    "        plt.title('Model Comparison - Test R² Score', fontsize=14)\n",
    "        plt.ylabel('R² Score', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # RMSE\n",
    "        plt.subplot(3, 1, 2)\n",
    "        bars = plt.bar(model_names, test_rmse, color='lightgreen')\n",
    "        bars[0].set_color('darkgreen')  # Highlight best model\n",
    "        plt.title('Model Comparison - Test RMSE', fontsize=14)\n",
    "        plt.ylabel('RMSE', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # MAE\n",
    "        plt.subplot(3, 1, 3)\n",
    "        bars = plt.bar(model_names, test_mae, color='salmon')\n",
    "        bars[0].set_color('darkred')  # Highlight best model\n",
    "        plt.title('Model Comparison - Test MAE', fontsize=14)\n",
    "        plt.ylabel('MAE', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../figures/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"✓ Created model comparison plots\")\n",
    "else:\n",
    "    print(\"Model predictions not found - skipping model performance visualization\")\n",
    "\n",
    "# 5.6 Feature importance visualization if available\n",
    "if 'feature_importances' in locals() and 'feature_names' in locals():\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "    sorted_features = [feature_names[i] for i in indices]\n",
    "    sorted_importances = feature_importances[indices]\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.barh(range(len(sorted_importances)), sorted_importances, align='center', color='lightblue')\n",
    "    plt.yticks(range(len(sorted_importances)), sorted_features)\n",
    "    plt.xlabel('Feature Importance', fontsize=12)\n",
    "    plt.title('Feature Importance for PM2.5 Prediction', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Created feature importance visualization\")\n",
    "elif 'best_model' in locals() and hasattr(best_model, 'feature_importances_'):\n",
    "    # Try to extract feature importances from the best model if available\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'Feature {i}' for i in range(X_train.shape[1])]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "    sorted_features = [feature_names[i] for i in indices]\n",
    "    sorted_importances = feature_importances[indices]\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.barh(range(len(sorted_importances)), sorted_importances, align='center', color='lightblue')\n",
    "    plt.yticks(range(len(sorted_importances)), sorted_features)\n",
    "    plt.xlabel('Feature Importance', fontsize=12)\n",
    "    plt.title('Feature Importance for PM2.5 Prediction', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Created feature importance visualization from best model\")\n",
    "else:\n",
    "    print(\"Feature importance information not found - skipping feature importance visualization\")\n",
    "\n",
    "print(\"\\n✅ Part 5 Completed: Advanced visualization and interpretation completed successfully\")\n",
    "print(\"All visualizations have been saved to the '../figures' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Part 6: Advanced Model Interpretation & Conclusions\n",
    "#############################################################\n",
    "\n",
    "print(\"## 6. Advanced Model Interpretation & Conclusions\")\n",
    "print(\"Implementing advanced model interpretation techniques and drawing final conclusions...\")\n",
    "\n",
    "# Import additional libraries if needed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Try to import SHAP for advanced model interpretation\n",
    "try:\n",
    "    import shap\n",
    "    shap_available = True\n",
    "    print(\"✓ SHAP library available for model interpretation\")\n",
    "except ImportError:\n",
    "    shap_available = False\n",
    "    print(\"⚠️ SHAP library not found. Install with 'pip install shap' for advanced model interpretation\")\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if not os.path.exists('../results'):\n",
    "    os.makedirs('../results')\n",
    "    print(\"Created '../results' directory for saving analysis results\")\n",
    "\n",
    "# 6.1 Advanced Model Interpretation with SHAP values\n",
    "if 'best_model' in locals() and 'X_test' in locals() and shap_available:\n",
    "    print(\"\\n6.1 Generating SHAP values for model interpretation...\")\n",
    "    \n",
    "    # Check if X_test is scaled\n",
    "    X_for_shap = X_test_scaled if 'X_test_scaled' in locals() else X_test\n",
    "    \n",
    "    # Convert to DataFrame if not already\n",
    "    if not isinstance(X_for_shap, pd.DataFrame):\n",
    "        try:\n",
    "            X_for_shap = pd.DataFrame(X_for_shap, columns=feature_names)\n",
    "        except:\n",
    "            feature_names = [f\"Feature_{i}\" for i in range(X_for_shap.shape[1])]\n",
    "            X_for_shap = pd.DataFrame(X_for_shap, columns=feature_names)\n",
    "    \n",
    "    # Create a prediction function that SHAP can use\n",
    "    def model_predict(X):\n",
    "        return best_model.predict(X)\n",
    "    \n",
    "    # Use KernelExplainer which works with any model\n",
    "    try:\n",
    "        # Choose a subset of data for background distribution (for efficiency)\n",
    "        background = shap.sample(X_for_shap, 100)\n",
    "        \n",
    "        # Create explainer using KernelExplainer which works with any model\n",
    "        explainer = shap.KernelExplainer(model_predict, background)\n",
    "        \n",
    "        # Calculate SHAP values for a subset of test data (for efficiency)\n",
    "        sample_size = min(100, len(X_for_shap))\n",
    "        shap_values = explainer.shap_values(X_for_shap.iloc[:sample_size])\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_for_shap.iloc[:sample_size], plot_type=\"bar\", show=False)\n",
    "        plt.title(\"Feature Importance (SHAP Values)\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../figures/shap_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Detailed SHAP plots\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        shap.summary_plot(shap_values, X_for_shap.iloc[:sample_size], show=False)\n",
    "        plt.title(\"SHAP Summary Plot\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../figures/shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Dependence plots for top 3 features (by average absolute SHAP value)\n",
    "        feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "        top_indices = np.argsort(feature_importance)[-3:]\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            feature_name = X_for_shap.columns[idx]\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.dependence_plot(\n",
    "                idx, shap_values, X_for_shap.iloc[:sample_size],\n",
    "                feature_names=list(X_for_shap.columns), show=False\n",
    "            )\n",
    "            plt.title(f\"SHAP Dependence Plot for {feature_name}\", fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'../figures/shap_dependence_{feature_name}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"✓ SHAP analysis completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error generating SHAP values: {e}\")\n",
    "        print(\"Continuing with alternative model interpretation methods...\")\n",
    "else:\n",
    "    print(\"⚠️ SHAP analysis skipped (missing model, test data, or SHAP library)\")\n",
    "\n",
    "# 6.2 Partial Dependence Plots (PDPs) for interpreting feature effects\n",
    "if 'best_model' in locals() and 'X_test' in locals():\n",
    "    print(\"\\n6.2 Creating Partial Dependence Plots...\")\n",
    "    \n",
    "    try:\n",
    "        from sklearn.inspection import PartialDependenceDisplay\n",
    "        \n",
    "        # Get feature names\n",
    "        if 'X_test_scaled' in locals() and hasattr(X_test_scaled, 'columns'):\n",
    "            feature_names = X_test_scaled.columns\n",
    "        elif hasattr(X_test, 'columns'):\n",
    "            feature_names = X_test.columns\n",
    "        else:\n",
    "            feature_names = [f\"Feature_{i}\" for i in range(X_test.shape[1])]\n",
    "        \n",
    "        # Determine top features - prioritize feature importance if available\n",
    "        if 'feature_importances' in locals():\n",
    "            top_indices = np.argsort(feature_importances)[-4:]\n",
    "        elif hasattr(best_model, 'feature_importances_'):\n",
    "            top_indices = np.argsort(best_model.feature_importances_)[-4:]\n",
    "        else:\n",
    "            # Without feature importances, use first few features\n",
    "            top_indices = range(min(4, len(feature_names)))\n",
    "        \n",
    "        # Create PDPs for top features\n",
    "        X_for_pdp = X_test_scaled if 'X_test_scaled' in locals() else X_test\n",
    "        \n",
    "        # Generate PDPs\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        display = PartialDependenceDisplay.from_estimator(\n",
    "            best_model, X_for_pdp, top_indices, \n",
    "            feature_names=feature_names,\n",
    "            ax=ax, kind='average'\n",
    "        )\n",
    "        plt.suptitle('Partial Dependence Plots for Top Features', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../figures/partial_dependence_plots.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✓ Partial Dependence Plots created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error creating Partial Dependence Plots: {e}\")\n",
    "\n",
    "# Save the best model if available\n",
    "if 'best_model' in locals():\n",
    "    try:\n",
    "        import joblib\n",
    "        \n",
    "        # Create models directory if it doesn't exist\n",
    "        if not os.path.exists('../models'):\n",
    "            os.makedirs('../models')\n",
    "            print(\"Created '../models' directory for saving trained models\")\n",
    "        \n",
    "        # Save the model\n",
    "        model_filename = f\"../models/best_model_{best_model_name if 'best_model_name' in locals() else 'final'}.joblib\"\n",
    "        joblib.dump(best_model, model_filename)\n",
    "        print(f\"✓ Best model saved to '{model_filename}'\")\n",
    "        \n",
    "        # Save the scaler if available\n",
    "        if 'scaler' in locals():\n",
    "            scaler_filename = \"../models/scaler.joblib\"\n",
    "            joblib.dump(scaler, scaler_filename)\n",
    "            print(f\"✓ Scaler saved to '{scaler_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error saving model: {e}\")\n",
    "\n",
    "print(\"\\n✅ Part 6 Completed: Advanced model interpretation completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
